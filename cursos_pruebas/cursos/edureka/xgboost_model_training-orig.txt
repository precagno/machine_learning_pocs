%conda install -c conda-forge shap
%conda update -n base -c conda-forge conda

-----

import shap ##builtin datasets

X, y = shap.datasets.adult()
X_display, y_display = shap.datasets.adult(display=True)
feature_names = list(X.columns)
feature_names

-----

display(X.describe())
hist = X.hist(bins = 30, sharey = True,figsize = (20,10))

-----

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
X_train_display = X_display.loc[X_train.index]

-----

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)
X_train_display = X_display.loc[X_train.index]
X_val_display = X_display.loc[X_val.index]

-----

import pandas as pd
train = pd.concat([pd.Series(y_train, index=X_train.index, name='Income>50K',dtype=int), X_train],axis=1)

validation = pd.concat([pd.Series(y_val, index=X_val.index, name='Income>50K',dtype=int), X_val],axis=1)

test = pd.concat([pd.Series(y_test, index=X_test.index, name='Income>50K',dtype=int), X_test],axis=1)

-----

train

-----

test

-----

## la primer columna será utilizada como la columna de salida
train.to_csv('train.csv', index = False, header = False)
validation.to_csv('validation.csv', index = False, header=False)

-----

import sagemaker, boto3, os
bucket = sagemaker.Session().default_bucket
prefix  = "demo-sagemaker-xgboost-adult-income-prediction"

boto3.Session().resource('s3').Bucket(bucket).Object(
	os.path.join(prefix, 'data/train.csv')).upload_file('train.csv')
boto3.Session().resource('s3').Bucket(bucket).Object(
	os.path.join(prefix, 'data/validation.csv')).upload_file('validation.csv')
	
-----

! aws s3 ls {bucket}/prefix/data --recursive

-----

import sagemaker

region = sagemaker.Session().boto_region_name
print("AWS Region: {}".format(region))

role = sagemaker.get_execution_role()
print("RoleArn: {}".format(role))

-----	

sagemaker.__version__   ## if sagemaker version > 2.20 OK

-----

from sagemaker.debugger import Rule, rule_configs
from sagemaker.session import TrainingInput

s3_output_location = 's3://{}/{}/{}'.format(bucket, prefix, 'xgboost_model')

container = sagemaker.image_uris.retrieve("xgboost", region, "1.2-1")
print(container)

xgb_model = sagemaker.estimator.Estimator(
	image_uri = container,
	role = role,
	instance_count = 1,
	instance_type = 'm1.m4.xlarge',
	volume_size = 5,
	output_path = s3_output_location,
	sagemaker_session = sagemaker.Session(),
	rules=[Rule.sagemaker(rule_configs.create_xgboost_report())]
	
)

##revisar sí instance_type = 'm1.m4.xlarge' es la instancia gratuita

-----

xgb_model.set_hyperparameters(
	max_depth = 5,
	eta = 0.2,
	gamma = 4,
	min_child_weight = 6,
	subsample = 0.7,
	objective = "binary_logistic",
	num_round = 1000
)

-----

from sagemaker.session import TrainingInput

train_input = TriningInput(
	"s3://{}/{}/{}".format(bucket, prefix, "data/train.csv"), content_type="csv"
)

validation_input = TriningInput(
	"s3://{}/{}/{}".format(bucket, prefix, "data/validation.csv"), content_type="csv"
)

-----

xgb_model.fit({"train": train_input, "validation": validation_input}, wait = True)

-----

role_output_path = xgb_model.output_path + "/" + xgb_model.latest_training_job.name + "/rule-output"
! aws s3 ls {rule_output_path} --recursive

------

! aws s3 cp {rule_output_path} ./ --recursive

------

from IPython.display import FileLink, FileLinks

display("Click link below to view the XGBoost Training report", FileLink("CreateXgboostReport/xgboost_report.html"))

------

profiler_report_name = [rule["RuleConfigurationName"]
			 for rule in xgb_model.latest_training_job.rule_job_summary()
			 if "Profiler" in rule["RuleConfigurationName"]][0]			 
profiler_report_name
display("Click link below to view the profiler report", FileLink(profiler_report_name+"/profiler-output/profiles-report.html"))


------

xgb_model.model_data

------

import sagemaker
from sagemaker.serializers impor CSVSerializer
xgb_predictor = xgb_model.deploy(
	initial_instance_count = 1,
	instance_type = 'ml.t2.medium',
	serializer = CSVSerializer()
)

------

xgb_predictor.endpoint_name

------

## Evaluate the model

import numpy as np
def predict(data, rows=1000):
	split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))
	predictions = ''
	for array in split_array:
	predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf8')])
	return np.fromstring(predictions[1:], sep=','


-------
	
import matplotlib.pyplot as plt

predictions = predict(test.to_numpy()[:,1:])
plt.hist(predictions)
plt.show()

-------

import sklearn

cutoff = 0.5
print(sklearn.metrics.confusion_matrix(test.iloc[:,0], np.where(predictions > cutoff, 1, 0)))

print(sklearn.metrics.classification_reports(test.iloc[:,0], np.where(predictions > cutoff, 1, 0)))

---------

import matplotlib.pyplot as plt

cutoffs = np.arrange(0.01, 1, 0.01)
log_loss = []
for c in cutoffs:
	log_loss.append(
	sklearn.metrics.log_loss(test.iloc[:,0], np.where(predictions > c, 1, 0))
	)

plt.figure(figsize=(15,10))
plt.plot(cutoffs, log_loss)
plt.xlabel("Cutoff")
plt.ylabel("Log loss")
plt.show()


-------

print('Log loss is minimized at a cutoff of ', cutoffs[np.argmin(log_loss)],', and the log loss value at the minimun is', np.min(log_loss))



















